<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN"
                      "http://www.w3c.org/TR/html4/loose.dtd">
<html>

<head>
<title>OpenAFS Development Wish List / Road Map</title>
<link href="/openafs.css" rel="stylesheet" type="text/css">
</head>

<body bgcolor="#ffffff" text="#000000" link="#000fff" vlink="#0000ff" alink="#0000ff">

<h1 align="center">Development Wish List / Road Map for OpenAFS</h1>
<p>In its nine plus years as an open source project, OpenAFS has established AFS 
as one of the open source success stories.&nbsp; OpenAFS provides clients for all 
of the major operating system distributions and servers on all UNIX/Linux variants.&nbsp; 
Even so, there is still a great deal that could be done to turn AFS into a first 
class file system, especially for MacOS X and Microsoft Windows end users and 
future platform support.</p>
<p>The work to be accomplished on OpenAFS falls into six broad categories:</p>
<ul>
	<li><a href="#core">Core Client Functionality</a></li>
	<li><a href="#HI">Human Interfaces</a></li>
	<li><a href="#protocol">AFSv3 Remote Procedure Call Extensions</a></li>
	<li><a href="#server">Server Enhancements and Management Tools</a></li>
	<li><a href="#HPC">High Performance Computing Extensions</a></li>
	<li><a href="#misc">Miscellaneous</a></li>
	<li><a href="#documentation">Documentation</a></li>
</ul>
<p>It is the goal of the OpenAFS Elders to raise resources from the OpenAFS Community 
and others to successfully implement all of these functions over the next three 
to five years.</p>
<p>An <a href="#release_schedule">Implementation and Release Schedule</a> is 
provided at the end of the page.</p>
<p>A note about the estimates provided on this page.&nbsp; For many of the 
projects the Gatekeepers have designs, partially completed work, or even fully 
implemented systems.&nbsp; The estimates that are provided is the time necessary 
to complete the project and/or integrate it into a standard release of OpenAFS.</p>
<h2 wrap><a name="core"></a>1. Core Client Functionality</h2>
<p wrap>Core client functionality encompasses the AFS cache manager, file system 
interfaces, pioctl interfaces, and credential management.</p>
<h3 wrap>Microsoft Windows:</h3>
<p wrap>The Microsoft Windows client has received significant attention over the 
last four years.&nbsp; It is a fully functional client that works on all Microsoft 
Windows releases from Windows 2000 SP4 through Windows Vista and Server 2008.&nbsp; For a summary, 
see the <a href="http://www.secure-endpoints.com/#oafw_status_report">OpenAFS for 
Windows Status Report</a>.&nbsp; Still, there are a number of deficiencies that 
adversely impact the ability of end users to use AFS to its full existing potential.</p>
<ul>
	<li>
	<p wrap><strong>Read-only or Read-write disconnected mode:</strong><br>
	Microsoft Windows users are used to the &quot;Windows Offline Folders&quot; functionality 
	which permits them to synchronize local copies of files or folders from a CIFS 
	server to their local disk for use when disconnected from the network.&nbsp; 
	UMichigan long ago implemented a read-write disconnected mode for the UNIX AFS 
	client which permits users to continue using data within the AFS cache while 
	in an offline mode.&nbsp; Once the client is restored to an online state the 
	modifications made to the cache buffers are written back to the file server 
	provided that there are no conflicts.&nbsp;&nbsp; If there are conflicts a manual 
	conflict resolution process must be initiated.&nbsp; Conflict resolution is 
	hard but AFS users would gain a great deal even if the contents of the AFS cache 
	were available when disconnected from the file servers in a read-only manner.<br>
	Estimate: 1 month</p>
	</li>
	<!-- Implemented	
	<li>
	<p wrap><b>Executable pre-fetching:</b><br>
	The Windows client is often used to run applications out of the AFS name space.&nbsp;&nbsp; 
	When a file server becomes unreachable while an application is running it will 
	crash if it attempts to load a page that is not currently in the AFS cache.&nbsp; 
	This problem can be mitigated by modifying the AFS client to pre-fetch the entire 
	file if the file is likely to be an executable.&nbsp;<br>
	Estimate: 2 days </p>
	</li>
-->
	<li>
	<p wrap><strong>Native file system driver interface (aka IFS):</strong> <br>
	The current OpenAFS client on Windows is not a true Windows file system.&nbsp; 
	Instead it operates as a SMB translator service.&nbsp; The Windows OpenAFS client 
	creates a SMB fileserver on the client machine, and Windows accesses this SMB 
	server as a normal Windows shared volume.&nbsp; For each I/O operation made 
	to this virtual SMB server, the OpenAFS client translates the SMB request into 
	a comparable operation on the OpenAFS fileserver.&nbsp; This impacts the Windows 
	OpenAFS client in a number of negative ways:</p>
	<ul>
		<li>The semantics of the CIFS file system are different than the semantics 
		provided by OpenAFS.&nbsp; Because Windows sees the OpenAFS file system 
		as an CIFS share, it has no way of acquiring the true capabilities or semantics 
		of OpenAFS.&nbsp; This causes some applications to perform poorly when they 
		expect the semantics of CIFS, which OpenAFS does not necessarily provide.</li>
		<li>The use of the translator service requires data to be received by the 
		OpenAFS client via RX, translated into SMB packets, and then sent over the 
		virtual loopback interface to the actual SMB server on the same machine.&nbsp; 
		This results in a number of extra data copies, which greatly reduces OpenAFS 
		performance.&nbsp; Making OpenAFS a native Windows file system will reduce 
		the number of data copies and protocol translations, which will increase 
		performance.</li>
		<li>The CIFS/SMB protocol does not provide any mechanism for the server 
		to inform a client that an operation is actively being processed even if 
		it is taking a long time to complete.&nbsp; The CIFS clients in Windows 
		2000 and above implement a dynamic timeout algorithm that estimates how 
		long a request should take based upon the prior response time of the server 
		and the amount of data being transferred.&nbsp; As the OpenAFS SMB server 
		and cache manager are local to the machine, it is frequently the case that 
		the response time in on the order of hundreds of microseconds.&nbsp; When 
		a request to read or write large amounts of data from/to a file server occurs 
		or if the needed volume is temporarily busy, the CIFS client will frequently 
		timeout the request and tear down the SMB virtual circuit.&nbsp; This has 
		a negative impact on applications as it results in all file handles being 
		invalidated and all locks being dropped which must then be re-established.&nbsp;
		</li>
		<li>In order for the UNC server name &quot;AFS&quot; to be visible on all clients, 
		the Microsoft Loopback Adapter (MLA) must be installed in order to provide 
		a private network adapter to which the &quot;AFS&quot; Netbios name can be bound.&nbsp; 
		The installation of the MLA adversely effects several popular software licensing 
		and anti-spyware products which use the network adapter MAC address as a 
		unique key.</li>
	</ul>
	<p>The existing OpenAFS Client relies on an SMB server implementation (similar 
	to Samba) to export the AFS name space to Windows Applications.&nbsp; This has 
	a number of negative side effects that would be avoided if the OpenAFS for Windows 
	client were to be implemented via a combination of File System Redirector, File 
	System Filter driver, and a Network Provider.&nbsp; </p>
	<p>The solution is to replace the SMB server with a native File System 
	Redirector, a File System Filter driver, and a Network Provider that can be 
	supported on Windows XP SP2, Windows 2003 SP1, Windows XP 64, Windows 2003 
	R2, Windows Vista, Windows Server 2008, Windows 7, and Windows Server 2008 
	R2.<br>
	<a href="http://www.secure-endpoints.com/openafs-windows-roadmap.html#file system redirector">
	For more details ...</a><br>
	Estimate: December 2009</p>
	</li>
</ul>
<h3 wrap>All UNIX platforms:</h3>
<ul>
	<li><strong>split readonly/readwrite cache usage limits:</strong><br>
	Some usage scenarios make it desirable to ensure that a certain amount of the 
	available cache space is reserved for read-only data in order to reduce the 
	likelihood that the cached data will be swapped out.<br>
	(testing needed)<br></li>
	<li><strong>demand prefetch:</strong> <br>
	In order to reduce wait time while streaming files, it is desirable to allow 
	for a sliding prefetch window when a file is opened and being read. <br>
	(testing needed)<br></li>
	<li><strong>read/write disconnected AFS:</strong> <br>Read-only and partial 
	read-write operational support is present in the 1.5 release series.&nbsp; 
	Additional work needs to completed to support hard links, conflict 
	resolution, and pinning of objects, directory trees or volumes.<br>
	(substantially complete)<br></li>
	<li><strong>intermediate fileserver - client as fileserver:</strong><br>
	Each AFS cache contains up-to-date copies of data when a callback is registered 
	with a file server.&nbsp; In a cluster of clients that are reading the same 
	data over a low bandwidth network, it would be more efficient if the AFS cache 
	manager could obtain data from a nearby up-to-date copy as opposed to reading 
	it from the file server on the slow link.
	<br>Estimate: two to three months.<br></li>
	<li><strong>prevent repeated queries of the same volume location data:</strong><br>
	The UNIX client does not prevent multiple threads from repeatedly querying 
	the same volume location data.<br>&nbsp;</li>
	<li><strong>O_DIRECT and O_SYNC:</strong><br>
	These fopen flags should be supported.&nbsp; O_SYNC should force each write 
	to block until the data is sent to the file server.&nbsp; O_DIRECT should 
	perform a direct write to the file server bypassing the cache (and until 
	cache bypass is implemented, it should behave like O_SYNC.)</li>
</ul>
<h3 wrap>MacOS X:</h3>
<ul>
	<li>
	<p wrap>Provide for token acquisition as part of the logon process</p>
	</li>
	<li>
	<p wrap>PAG support</p>
	</li>
	<li>
	<p wrap>Re-implement Bulk Fetch Status calls in a manner compatible with the 
	MacOS X requirement that directory nodes be created as either &quot;directory&quot; or 
	&quot;file&quot; at allocation time.</p>
	</li>
	<li>
	<p wrap>Support cross-volume Finder &quot;drag&quot; moves</p>
	</li>
</ul>
<h3 wrap>Linux:</h3>
<ul>
	<li>
	<p wrap>On-going maintenance necessary to keep up with backward incompatible 
	changes to the Linux kernel and new interfaces provided and used therein.</p>
	</li>
</ul>
<h3 wrap>Solaris:</h3>
<ul>
	<li>
	<p wrap>update client to use system inodes instead of private inode pool.</p>
	</li>
</ul>
<h3 wrap>AIX:</h3>
<ul>
	<li>AIX 5 kcred PAG support</li>
</ul>
<h3>BSD:</h3>
<ul>
	<li>Port the cache manager to NetBSD<br>
	Estimate: 1 month</li>
</ul>
<h3>New Client Platforms:</h3>
<p>There is a growing demand for pervasive access to data from handset devices.&nbsp; 
Clients for Symbian S60, Windows Mobile, Apple&#39;s iPhone/iTouch,
<a title="OpenHandsetAlliance" href="http://www.openhandsetalliance.com/">
OpenHandsetAlliance</a> (aka Android), and Nokia Maemo devices will be critical in the 
years to come.</p>
<h3>All Platforms:</h3>
<ul>
	<li><p wrap><strong>Client Cache Usage Tracking and Tuning:</strong><br>
	The current cache managers implementation an explicit Least Recently Used algorithm for
recycling objects.   This algorithm does not take into account:
<ul>
 <li>frequency of use</li>
 <li>whether the file is currently open</li>
</ul> <br>
	By tracking the right statistics, the cache manager would be more
likely to have the right data cached.  One of the extreme examples
that has been discussed is a maintenance application that runs five nights a
week at 1am on tens of thousands of machines.&nbsp; The running of this application pounds the volume 
	and file server because every machine must reload the application contents 
	into the cache at the same time. If the cache manager could
predict this utilization based upon past history, then the clients could
slowly prefetch the data ahead of time easing the load on the file
servers and improving the performance of the application on the local
machine.</li>
</ul>
<h2 wrap><a name="HI"></a>2. Human Interfaces </h2>
<p wrap>There have been many discussions about how hard AFS is to use, how end users 
don&#39;t want AFS and really want a WebDAV solution.&nbsp;&nbsp; What do those statements 
really mean?&nbsp; First, AFS isn&#39;t any harder to use than any other authenticated 
file system from the perspective of end users.&nbsp; If a user has an &quot;encrypted&quot; 
local disk she has to authenticate herself by providing her password.&nbsp; With 
the single sign-on solutions available for OpenAFS there isn&#39;t much reason 
for users today to be running without tokens when they have network access.&nbsp; 
Second, the statement that end users don&#39;t want AFS (as opposed to some other centralize 
storage solution) really makes no sense.&nbsp; End users don&#39;t ask for technologies, 
they ask for functionality.&nbsp; If a user wants centralized storage then the user 
wants centralized storage.&nbsp; </p>
<p wrap>Users describe their desires using the technologies that are most 
familiar to them which today most often means Windows Shares (CIFS) and Browser 
based services.&nbsp; Why?&nbsp; Because those are the technologies the user is 
presented on his or her operating system&#39;s desktop.&nbsp; The vast majority of 
daily users are uncomfortable with command-line operations.&nbsp; Improving the 
ease of use of AFS can be achieved by providing tighter integration with the 
operating system desktop environment.&nbsp; </p>
<h3>Microsoft Windows:</h3>
<p>The Secure Endpoints Inc.
<a href="http://www.secure-endpoints.com/openafs-windows-roadmap.html"><em>OpenAFS 
Windows Road Map</em> web page</a> provides an number of mock ups of Explorer Shell 
extensions that can be used to not only make AFS more accessible to end users 
but also significantly improve its ease of use.&nbsp; By making the Explorer Shell 
AFS aware, users will be more comfortable using it. No longer will users 
have to use command line techniques to access AFS and manage its contents and metadata.
</p>
<p>One of the most important ideas that was the result of discussions with Stanford 
University&#39;s Help Desk staff is the concept of Custom Name Spaces.&nbsp; On Microsoft 
Windows a Name Space is a virtual folder that appears as part of the Explorer Shell.&nbsp; 
The objects &quot;My Computer&quot;, &quot;My Documents&quot;, &quot;Control Panel&quot;, My Network Places&quot;, 
&quot;My Sharing Folders&quot;, etc. are all name spaces.&nbsp; Stanford University has been 
shipping for many years variations of an application now called &quot;Stanford Desktop 
Tools&quot;.&nbsp; One of the features of SDT is the ability to search for classes, users, 
departments, and projects and map a drive letter to the associated AFS volume.&nbsp; 
Another feature is the ability to quickly map a drive letter to &quot;my home directory&quot;.&nbsp; 
A final feature is the most recently used volume list. </p>
<p>With Name Spaces, we can implement all of this functionality.&nbsp; We can define 
a &quot;recently used volumes list&quot; which is always populated with the volumes the user 
most recently read or stored data to.&nbsp; We can define a &quot;My Stanford Home Directory&quot; 
name space that always contains a shortcut to the volume associated with the user&#39;s 
token for the ir.stanford.edu cell.&nbsp; We can also create name spaces for &quot;Stanford 
Users&quot;, &quot;Stanford Classes&quot;, &quot;Stanford Departments&quot;, etc.&nbsp; Other organizations 
can distribute their own AFS name spaces that represent important data that is stored 
in their cell.&nbsp; AFS name spaces from multiple organizations can co-exist on 
the same system.&nbsp; Since name spaces are built into the Explorer Shell they 
are always easily accessible to the end user because they become a part of the Desktop.</p>
<p>
<a href="https://www.secure-endpoints.com/netidmgr/proposal-afs-namespace.pdf">A 
detailed proposal describing an AFS Name Spaces implementation is available in PDF.</a></p>
<p>Users expect to find a Control Panel for Services that support per-user configuration.&nbsp; 
For OpenAFS users can configure the behavior of the AFS Credential Provider for 
Network Identity Manager and their Protection Service Groups.&nbsp;
<a href="http://www.secure-endpoints.com/openafs-windows-roadmap.html#control panel">
For more details ...</a></p>
<p>System-wide configuration of Services are performed via Microsoft Management 
Console plug-ins.&nbsp;
<a href="http://www.secure-endpoints.com/openafs-windows-roadmap.html#client service mmc">
For more details ...</a></p>
<p>Microsoft Windows Vista User Account Control Privilege Separation.&nbsp;
<a href="http://www.secure-endpoints.com/openafs-windows-roadmap.html#vista_uac">
For more details ...</a></p>
<h3>MacOS X:</h3>
<p>Apple doesn&#39;t permit the same degree of customization of the Finder as Microsoft 
does for the Explorer Shell.&nbsp; However, the Finder can be customized with an 
AFS virtual folder and AFS context menus. Likewise, certain other graphical interfaces 
which will become available in Leopard provide opportunities for customization to 
ease use of AFS.</p>
<ul>
	<li>
	<p wrap>Enhance Finder with an OpenAFS Context menu</p>
	</li>
</ul>
<h2 wrap><a name="protocol"></a>3. AFS3 Protocol Feature Enhancements </h2>
<p wrap>In order for AFS to be treated as a first class file system for MacOS X 
and Microsoft Windows it must gain the following functionality: </p>
<ul>
	<li>
	<p wrap><strong>Removing Directory Limitations:</strong><br>
	The current AFS directory format and RPCs suffer from a number of limitations 
	that adversely affect the user experience.&nbsp; A directory has a maximum of 
	64,000 entries if all file names are 16 or fewer octets.&nbsp; Longer names are 
	implemented by consuming an additional entry for each additional 32 octets of file name.&nbsp; 
	Given the ever increasing length of file names some cells are filling the directory 
	with as few as 10,000 entries.&nbsp;&nbsp; Some scientific research projects 
	require the use of millions of files perhaps containing a single 
	data byte within a single directory.</p>
	<p>The current AFS directory format is very inefficient for searching when 
	case-insensitivity or Unicode normalization is required.&nbsp; 
	Under these circumstances search time is linear to the number of entries in 
	the directory.&nbsp; Many modern file systems implement the directory 
	as a B+ tree to permit O(log n) searching.&nbsp; The existing format places 
	a heavy burden on each and every cache manager.&nbsp; Each client must download 
	a copy of the directory buffers and perform linear searching.&nbsp; This results 
	in heavy use of the CPU when searching directories with 500 or more entries.</p>
	<p>Another issue is the lack of support for internationalization.&nbsp; In 
	the current directory format directory entries are stores as a sequence of 
	octets without any character set hinting.&nbsp; A file that is stored using 
	a name encoded with ISO 8859-5 or CP437 will not be represented correctly to 
	the user on a system that expects UTF-8.&nbsp; Even when file names are 
	stored using UTF-8 it is important to recognize that 
	depending on the input mechanisms it is possible for a user to enter the same 
	semantic string using different octet sequences.&nbsp; Therefore it 
	is crucial that any implementation of Unicode file names support
	<a href="http://en.wikipedia.org/wiki/Unicode_normalization">normalized forms</a> 
	for comparison.&nbsp;&nbsp;&nbsp;</p>
	<p>Finally, Microsoft Windows and MacOS X are now requiring that first class 
	file systems support the concept of multiple data streams per file.&nbsp; These 
	streams are used to store extended attributes, security zone information, resource 
	forks, and other forms of meta data in addition to providing a general purpose 
	storage mechanism for applications.&nbsp;
	<a href="http://en.wikipedia.org/wiki/Fork_(filesystem)">For more details ...</a></p>
	<p>At the 2004 AFS Hackathon in Stockholm there was much discussion of potential 
	methods of extending the existing directory format to support Unicode.&nbsp;&nbsp;
	<a href="http://www.afsig.se/afsig/space/AFS+directory+format+extensions">http://www.afsig.se/afsig/space/AFS+directory+format+extensions</a>&nbsp; 
	However, these approaches did not address the directory search performance issues, 
	the entry limitations or multiple data streams.&nbsp; </p>
	<p>The current direction under consideration is to completely replace the on 
	disk directory format with an entirely new one consisting of data blocks representing 
	nodes in a B+ tree with each block containing a variable number of entries.&nbsp; 
	The new data structure would be Unicode aware and support multiple data streams.&nbsp; 
	Microsoft Windows clients would implement extended attributes in a reserved 
	data stream.&nbsp; MacOS X clients would use a reserved stream for the resource 
	fork.</p>
	<p>New versions of all of the directory RPCs would be implemented to support 
	the new data structure.&nbsp; Clients that use the new APIs would be delivered 
	directory buffers which construct a B+ tree which in turn would significantly 
	improve directory search times.&nbsp; </p>
	<p>For old clients, new implementations of the old RPCs would deliver directory 
	data translated to the old linear format up to the maximum number of directory 
	entries.&nbsp; It is possible that old clients will not be able to see all the 
	files in a given directory.<br><br>Implementation promised by Your File System, 2011.
	&nbsp; <br>
	</p>
	</li>
	<li>
	<p wrap><strong>Extended Attributes:</strong><br>
	Extended Attributes are used by MacOS X to store resources and DOS 
	Attributes.&nbsp; When they are not supported by the file system, MacOS X is 
	forced to create the ._ (Apple Double files).&nbsp; Extended Attributes on 
	Microsoft Windows are used to store a variety of meta-data about files and 
	directories.&nbsp; The lack of EA support in AFS damages the Windows User 
	Experience.&nbsp; AFS Cache Managers can implement support for extended 
	attributes and store them in hidden Apple Double files while waiting for 
	full EA support within AFS volumes.<br>
	<a href="http://en.wikipedia.org/wiki/Extended_file_attributes">For more details 
	...</a></p>
	Implementation promised by Your File System, 2011.
	<br>
	</li>
	<li>
	<p wrap><strong>Per-file ACLs:</strong><br>AFS supports per-directory ACLs.&nbsp; Per-file ACLs would make it possible 
	to apply a different set of access constraints on a single object within a directory.&nbsp; 
	At the present time storing multiple objects with different access controls 
	requires that they be stored in separate directories. The AFS protocol provides 
	partial support for this from the AFS/DFS translator, and this is supported 
	in clients going back to IBM AFS. </p>
	Implementation promised by Your File System, 2011.
	<br></li>
	<li>
	<p wrap><strong>Mandatory Locking and Byte range locks:<br></strong>Platforms such as Microsoft Windows and MacOS X require that their first 
	class file systems support mandatory lock semantics and byte ranges.&nbsp; Applications 
	which rely on these capabilities such as Microsoft Office and databases risk 
	data corruption if their data files are altered while they are assumed to be 
	under a lock.&nbsp;&nbsp; AFS only provides advisory full file locks and provides 
	no upgradeable lock type.&nbsp; The existing AFS file server lock implementation 
	doesn&#39;t keep track of which clients were issued locks which results a number 
	of situations in which lock counts can become incorrect and produce a denial 
	of service on a given file.</p>
	<p>The Windows AFS client in the 1.5 series has added a localized implementation 
	of mandatory locking and byte range locks.&nbsp; Each time an application requests 
	a byte range to be locked, the cache manager ensures that it has an appropriate 
	full lock on the object.&nbsp; The cache manager than accepts the responsibility 
	of tracking each of the locks and doling out a range at a time.<br>Estimate: 2 months. 
	<br><br>Implementation promised by Your File System, 2010</p>
	</li>
	<li>
	<p><strong>Status Data (Callback Registration) Expiration Algorithm 
	Improvements:</strong><br>Status data and callback registration expiration is currently determined 
	based upon the number of clients that are accessing the data instead of the 
	likelihood that the data is going to change. <br><br>Implementation in 
	progress.&nbsp;&nbsp; Awaiting Standardization.</p>
	</li>
	<li>
	<p><strong>OPEN/CLOSE File Server RPCs:</strong><br>New file server RPCs would provide new audit data<br>
	<br>Implementation promised by Your File System, 2010</p>
	</li>
	
</ul>
<h2 wrap><a name="server"></a>4. Server Enhancements and Management Tools</h2>
<h3 wrap>All Platforms </h3>
<ul>
	<li><strong>LDAP integration:</strong><br>
	Protection Server to LDAP Proxy implementation and Direct File Server to LDAP 
	implementation.&nbsp; Separate schemas will be necessary depending on whether 
	the LDAP server is OpenLDAP or Active Directory.<br>
	<br>
	Brett Trotter <a class="moz-txt-link-rfc2396E" href="mailto:blt@iastate.edu">
	&lt;blt@iastate.edu&gt;</a> wrote an implementation called <strong>ptsldap</strong>.&nbsp; In 
	July 2005 he had working code using the Mozilla LDAP library and was porting 
	it to use OpenLDAP instead.<p>Additionally, Volker Lendecke implemented a similar 
	project for use with Samba, which is not known to have been completed.</p>
	<p>Luke Howard <a href="http://www.padl.com">PADL Ltd.</a>) developed an AFS 
	Protection Service as part of his Active Directory clone, XAD.&nbsp; Ownership 
	of XAD has since been transferred to Novell.&nbsp; However, it is expected that 
	Luke will assist us in developing a new implementation in the coming months.</p>
	</li>
	<li><strong>Multiple back-end file server:</strong><br>
	Current file servers must be built to support one type of file partition, either
	<em>inode</em> or <em>namei</em>.&nbsp; This makes it impossible to mix partition 
	types on the same machine for the purpose of performance comparison or conversion. 
	This project will make the storage back end modular, and allow for additional back ends to be provided.<br>
	Estimate: 1 month<br>
	<br>
	</li>
	<li><strong>Posix Extended Attribute File Server Back-end:</strong><br>
	Using Posix Extended Attributes to store AFS metadata will provide the 
	portability of the <em>namei</em> backend with the performance of the <em>
	inode</em> backend.&nbsp;&nbsp; A <em>PosixEA</em> backend would not require 
	a special fsck which would permit the use of journal logs.<br>
	<br>Implementation promised by Your File System, 2011
	<br>
	</li>
	<li><strong>Demand Attach File Server:</strong><br>
	The Demand Attach File Server is a compile time option that produces a file 
	server that supports several new and important features:<br>
	<br>
&nbsp; * an enhanced volume management library that supports:<br>
&nbsp;&nbsp;&nbsp; . lock-less I/O<br>
&nbsp;&nbsp;&nbsp; . on-demand attachment of volumes<br>
&nbsp;&nbsp;&nbsp; . parallel shutdown of the file server<br>
&nbsp;&nbsp;&nbsp; . on-line salvaging of volumes<br>
&nbsp;&nbsp;&nbsp; . automatic detachment of inactive volumes<br>
	<br>
&nbsp;* a new salvageserver daemon which can salvage volumes on-demand<br>
	<br>
&nbsp;* a modified bos and bosserver<br>
&nbsp;&nbsp;&nbsp; . fileserver state saving and restoration<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - host state<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - callback state<br>
	<br>
	With the DAFS, shutdown and startup of the file server is significantly 
	faster.&nbsp; Instead of salvaging a partition at a time, individual volumes 
	are salvaged in the background while volumes that do not need salvaging are 
	served by the fileserver to clients.&nbsp; Volumes are attached on-demand 
	and detached when no longer required.&nbsp; This reduces the number of 
	volumes that may require salvaging upon a restart.<br>
	<br>
	Implemented in 1.5 release series.&nbsp;&nbsp; Requires Testing<br>Target: 
	1.6<br>
	<br>
	</li>
	<li><strong>Multi-Protocol File Server:</strong><br>
	The current AFS file server can only communicate using the AFS3 protocol.&nbsp; 
	Once all of the functionality in section 3, AFS Protocol Feature 
	Enhancements, is implemented, the file server will support all of the 
	capabilities necessary to alternative protocol front ends including CIFS/SMB2.0 and WebDAV.<br>
	<br>
	Estimated: To be determined.<br>
	&nbsp;
	</li>
	<li><strong>Protection Server enhancements for alternate principal names:</strong><br>
	The AFS Protection Service maintains a database that maps user names to AFS 
	IDs.&nbsp; This table needs to be extended to a many-to-one relationship of 
	user names to AFS IDs.&nbsp; In addition, multiple name forms must be supported 
	in order to permit different types of user authentication.<br>
	A proposed implementation was sketched out at the AFSig Hackathon
	in Stockholm in 2004 (<A HREF="http://web.archive.org/web/20060211111127/http://www.afsig.se/snipsnap/space/prdb+extensions">details</A>)
	<br>
	<p>Estimate: 2.5 to 3.5 months. Implementation promised by Your File System, 2010</p>
	</li>
	<li><strong>Volume Server enhancements for large volume names:</strong><br>
	Volume names within the existing RPCs and database are too short given the number 
	of volumes that are now being deployed in existing cells.<br>
	<br>Estimate: 1 week<br>
	Implementation promised by Your File System, 2011
	<br>
	<br>
	</li>
	<li><strong>Volume Server enhancements for split horizon addressing:</strong><br>
	Many sites now provide services from hosts which exist behind a NAT to hosts 
	both inside and outside that NAT. The vlserver should provide interfaces to 
	allow returning only some addresses to queries when responding to volume location 
	requests. <br>
	  <br>Implementation promised by Your File System, 2011
	<br>
	<br>
	</li>
	<li><strong>Volume Server read-only replication optimizations:</strong><br>
	A volume release current requires that the entire file be copied from the
	server maintaining the release clone to the replication site instead of only 
	copying modified chunks.  This is a significant performance issue especially
	as 64-bit file sizes and append only files become more common.  Modifying
	the volume replication process would provide significant clock time and 
	network traffic savings.&nbsp; Possible approaches include:<ul>
		<li>data compression of the volume dumps can reduce network bandwidth and 
	storage requirements</li>
		<li><a href="http://en.wikipedia.org/wiki/Rsync">rsync/rdiff</a> style 
	replication can be used to reduce the cost of replicating large files that 
	have only had small changes since the previous replica was created.&nbsp; 
	This can be coupled with additional read-only clone revisions and volume 
	version numbers to produce very rdiff streams that can be used to update 
	&quot;old release&quot; replicas without requiring a full volume replication.</li>
		<li>encrypted volume dumps can increase security when storing dump files in 
	external backup systems</li>
	</ul>
	<br>
	Estimate: 2 to 3 months<br>Implementation promised by Your File System, 2011&nbsp;
	</li>
</ul>
<ul>
	<li><strong>Read/write volumes and replication:</strong><br>
	After optimizing the size of volume dumps it should be possible to implement 
	a simple read/write replica model in which there is a single master for each 
	volume to be used for writes and locking and multiple replicas that can be 
	used for reads.</li>
	<br>
	<br>
	Implementation promised by Your File System, 2011
	<br><br>
	<li><strong>Partition UUIDs:</strong><br>
	Assigning UUIDs to each partition instead of the exsting non-unique names 
	will permit drives to be migrated between machines with just a simple update 
	of the volume database before continued use.&nbsp; This will eliminate most 
	of the work associated with synchronizing the database and removing stale 
	entries.<br>
	Estimate: 1 to 2 weeks<br>Implementation promised by Your File System, 2011
	<br>
	<br>
	</li>
	<li><strong>Machine Accounts in the Protection Database:</strong><br>
	Add the concept of machine accounts to the protection server database 
	permitting authenticated machine access that does not result in the client 
	being treated as a member of system:authuser.<br>
	  Implementation promised by Your File System, 2010<br><br>
	<li><strong>UBIK improvements or replacements:</strong><br>
	UBIK is the database replication protocol used by AFS server for the 
	synchronizing the volume and protection databases among servers.&nbsp; It is 
	an elected single master model in which the selection of the master is 
	determined strictly based upon the IP addresses of the server subset which 
	are capable of mutual communication and which are sufficient to establish 
	quorum.&nbsp; The server with the lowest IP address is elected master.&nbsp; 
	There are a number of scalability and performance issues with UBIK that must 
	be addressed.<ul>
		<li><strong>Master Selection Algorithms</strong><br>
		Master selection should be configurable based upon the needs of the 
		organization and not tied to the IP addressing.&nbsp; One model involves 
		configuring additional state information including priorities based upon 
		the servers in the quorum, the time of data, source of network traffic, 
		etc.&nbsp; It should be possible a master to be selected with a quorum 
		that is not represented by a fully connected graph.</li>
		<li><strong>True Server Multi-homing:</strong><br>
		CellServDB changes are required </li>
	</ul>
	</li>
	<ul>
		<li><strong>pthreading:</strong><br>
		The new implementation should support the use of pthreads instead of lwp 
		in order to maximize performance of multiple processor systems.</li>
		<li><strong>Increase Record Sizes:</strong><br>
		The existing record size results in significant design restrictions and 
		imposes undesirable overhead.&nbsp; </li>
		<li><strong>Multi-master Replication:</strong><br>
		Single master replication is easier to implement than multiple master 
		replication models.&nbsp; However, multiple master replication makes the 
		most efficient use of network resources.</li>
	</ul>	
	</ul>
<h3 wrap>Microsoft Windows</h3>
<p wrap>Once AFS is capable of being used as a first class file system for Microsoft 
Windows clients it will make sense to support the AFS servers on the Windows Server 
platform as there are a large number of Microsoft Windows only IT organizations 
that do not have the expertise to manage UNIX/Linux systems.&nbsp; The servers are 
mostly there already.&nbsp; There is work that needs to be done on the NTFS Namei 
implementation and there needs to be much better integration with power management, 
plug-n-play networking, and Windows Event Logging.</p>
<p>Of course if you want to host services on Windows, you must provide a Microsoft 
Management Console plug-in to manage them.</p>
<p>The primary reason that we haven&#39;t spent the time and energy to get the AFS servers 
in tip top shape is that without the protocol feature enhancements, users that attempt 
to deploy AFS in an all Windows environment are bound to be disappointed.</p>
<p wrap>
<a href="http://www.secure-endpoints.com/openafs-windows-roadmap.html#afs servers">
For more details ...</a><br>
<br>
Estimate: 4 to 6 weeks</p>
<h2><a name="HPC"></a>5. High Performance Computing Extensions</h2>
<ul>
	<li><strong>RX/TCP and IPv6:<br>
	</strong>The networking protocol used by OpenAFS was developed in the late 1980s.&nbsp; 
	It is a Remote Procedure Call transport called Rx that provides a stream interface 
	that runs over UDP/IP.&nbsp; This protocol was designed to address the traditionally 
	poor performance of TCP implementations at the time, and to be scalable to large 
	numbers of clients.<p>Since that time, a large amount of networking research 
	has been done on TCP performance, and modern TCP implementations are capable 
	of good performance on high-speed networks.&nbsp; Also, other protocols such 
	as Infiniband and SCTP have emerged as alternative transports to TCP.&nbsp; 
	The growth of the World Wide Web has pushed operating system vendors to develop 
	interfaces that allow applications to manage thousands of clients simultaneously 
	in a scalable manner.&nbsp; While incremental improvements have been made to 
	the Rx protocol since it was first developed, it has not been able to take advantage 
	of the performance available in modern networks.</p>
	<p>Our proposal includes the following work items:/p>
	<p style="text-indent: -18pt; margin-left: 18pt;">
	<span style="font-family: Symbol;">·<span style="font-family: &quot;Times New Roman&quot;; font-style: normal; font-variant: normal; font-weight: normal; font-size: 7pt; line-height: normal; font-size-adjust: none; font-stretch: normal;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
	</span></span>Develop an Rx protocol that uses TCP as a transport (RxTCP).&nbsp; 
	As part of an earlier project, Kenneth Hornstein has developed a prototype of 
	RxTCP; our goal is to complete this work and integrate it properly into the 
	OpenAFS distribution.<br>
	<br>
	There are many challenges with this approach.&nbsp; Much of the OpenAFS code 
	has an implicit assumption that the transport protocol is the traditional Rx 
	interface.&nbsp; This is further complicated by the lack of any formal API for 
	Rx.&nbsp; As a result of this, many OpenAFS programs use what normally would 
	be considered internal interfaces, and perform internal operations such as directly 
	manipulate queues of packets.&nbsp; The Rx API has no way to indicate such things 
	as connection types, so new API functions will need to be created.&nbsp; The 
	Rx library makes heavy uses of threads, but uses two distinct threading libraries 
	(pthreads and a custom thread library called LWP).&nbsp; Any new transport must 
	not only be thread-safe, but also present the same threading model to applications 
	so that they do not need to be rewritten.&nbsp; To maintain compatibility with 
	existing clients and servers, the original UDP transport must still function 
	simultaneously with the new transport.<br>
	<br>
	Despite these obstacles, much work has been completed.&nbsp; The RxTCP transport 
	has been implemented, and tests have shown excellent performance on Gigabit 
	networks.&nbsp; The remaining challenge is to integrate this transport into 
	the actual OpenAFS clients and servers.&nbsp; This new transport protocol addresses 
	a number of deficiencies in the original Rx protocol and implementation:</p>
	<p style="text-indent: -18pt; margin-left: 72pt;">
	<span style="font-family: Courier New;">o<span style="font-family: &quot;Times New Roman&quot;; font-style: normal; font-variant: normal; font-weight: normal; font-size: 7pt; line-height: normal; font-size-adjust: none; font-stretch: normal;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
	</span></span>The original Rx protocol was (with one exception) limited in packet 
	size to one Ethernet MTU (1500 bytes).&nbsp; In general, research has shown 
	that larger packet sizes facilitate higher performance due to the overall reduction 
	in per-packet processing time and the need to process fewer packets in order 
	to send the same amount of data.&nbsp; With the current networking APIs available 
	to applications today, programs cannot query the size of the MTU on a networking 
	interface, nor can they determine the MTU of a particular network path (even 
	though the operating system may have determined that already).<br>
	<br>
	With the use of TCP, the operating system can make use of knowledge not available 
	to the application, such as accurate estimates of round trip time, network path 
	MTU, and interface MTU, and as a result can take advantage of the capabilities 
	of modern networks, such as Ethernet jumbo frames.&nbsp; The exception to this 
	1500 byte packet limit is that Rx has a concept called a “jumbogram”, which 
	places multiple Rx packets into one UDP datagram.&nbsp; Unfortunately, in practice 
	this results in no net gain in performance, since the Rx per-packet processing 
	is not reduced.</p>
	<p style="text-indent: -18pt; margin-left: 72pt;">
	<span style="font-family: Courier New;">o<span style="font-family: &quot;Times New Roman&quot;; font-style: normal; font-variant: normal; font-weight: normal; font-size: 7pt; line-height: normal; font-size-adjust: none; font-stretch: normal;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
	</span></span>Since Rx utilizes a datagram transport but provides a reliable 
	stream interface, it must implement many of the features that are already provided 
	by TCP, such as a windowing algorithm, packet loss detection and retransmission, 
	and congestion control.&nbsp; While incremental improvements have been made 
	to the Rx implementation since it was first developed, it has not received the 
	attention that TCP performance has had during the same time period.&nbsp; The 
	use of TCP as the basis for Rx allows us to leverage the serious engineering 
	work that has been done on TCP, rather than requiring that same level of effort 
	be placed into Rx.</p>
	<p style="text-indent: -18pt; margin-left: 72pt;">
	<span style="font-family: Courier New;">o<span style="font-family: &quot;Times New Roman&quot;; font-style: normal; font-variant: normal; font-weight: normal; font-size: 7pt; line-height: normal; font-size-adjust: none; font-stretch: normal;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
	</span></span>The Rx implementation is very large and monolithic.&nbsp; Part 
	of the size of Rx results in having to perform many of the same functions of 
	TCP.&nbsp; This size and complexity makes it extremely difficult to understand 
	and follow the code.&nbsp; In addition, most of the Rx work is done by a relatively 
	small number of extremely complicated functions.&nbsp; In addition to lacking 
	modularity, this makes profiling extremely difficult, as most of the time spent 
	by Rx takes place in few functions, and it becomes difficult to get further 
	granularity by profiling since most profiling tools operate on a function basis.<br>
	<br>
	In contrast, RxTCP has a much smaller implementation, and is very modular.&nbsp; 
	Since the overall complexity is reduced, this makes it easier to profile and 
	understand.</p>
	<p style="text-indent: -18pt; margin-left: 72pt;">
	<span style="font-family: Courier New;">o<span style="font-family: &quot;Times New Roman&quot;; font-style: normal; font-variant: normal; font-weight: normal; font-size: 7pt; line-height: normal; font-size-adjust: none; font-stretch: normal;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
	</span></span>All data transmitted or received by Rx is divided internally into 
	per-packet buffers, which means data that is delivered to an application must 
	undergo two copies to reach its destination (kernel to Rx buffers, Rx buffers 
	to application buffers).<br>
	<br>
	RxTCP eliminates this copy completely on writes; data is sent directly from 
	the application buffer to the network stack.&nbsp; On reads data is delivered 
	directly from the network stack to an application when an application buffer 
	is available; if an application buffer is not available, then data is placed 
	into a large contiguous buffer internal to Rx and copied out completely when 
	an application buffer becomes available.</p>
	<p style="text-indent: -18pt; margin-left: 72pt;">
	<span style="font-family: Courier New;">o<span style="font-family: &quot;Times New Roman&quot;; font-style: normal; font-variant: normal; font-weight: normal; font-size: 7pt; line-height: normal; font-size-adjust: none; font-stretch: normal;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
	</span></span>Since Rx is a custom protocol, any tool used to analyze Rx performance 
	must be designed specifically for Rx use, or we must make modifications to an 
	existing tool.&nbsp; With the use of TCP, we can use any one of a number of 
	off-the-shelf tools to analyze TCP performance.</p>
	<p style="text-indent: -18pt; margin-left: 18pt;">
	<span style="font-family: Symbol;">·<span style="font-family: &quot;Times New Roman&quot;; font-style: normal; font-variant: normal; font-weight: normal; font-size: 7pt; line-height: normal; font-size-adjust: none; font-stretch: normal;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
	</span></span>Analyze remaining AFS performance deficiencies.&nbsp; Under an 
	SBIR grant, Sine Nomine has already conducted a formal analysis of AFS performance.&nbsp; 
	Poor Rx behavior was identified as the number one item impacting performance.&nbsp; 
	Our strategy for dealing with Rx performance is to utilize RxTCP, as detailed 
	above.<br>
	<br>
	Aside from Rx, a number of other bottlenecks were identified in the UNIX OpenAFS 
	client.&nbsp;&nbsp; After the integration of RxTCP, we will perform another 
	series of benchmarks to mimic the ones in the Sine Nomine report to analyze 
	the issues reported by Sine Nomine, and to see if any new issues arise.&nbsp; 
	The issues identified by the Sine Nomine research were as follows:</p>
	<p style="text-indent: -18pt; margin-left: 72pt;">
	<span style="font-family: Courier New;">o<span style="font-family: &quot;Times New Roman&quot;; font-style: normal; font-variant: normal; font-weight: normal; font-size: 7pt; line-height: normal; font-size-adjust: none; font-stretch: normal;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
	</span></span>Write performance is hurt by UNIX OpenAFS’s sync-on-close semantics.&nbsp; 
	Currently the UNIX OpenAFS client performs data synchronization when a file 
	is closed by an application, or the client cache dirty blocks have exceeded 
	a high-water mark threshold.&nbsp; For large files, this results in large time 
	periods during writes where the network is quiescent, and the application must 
	wait either when the file is closed or during a write for all of the outstanding 
	data to be written to the fileserver.<br>
	<br>
	One proposal offered in the Sine Nomine paper was to relax the sync-on-close 
	semantics as offered by the AFS client today.&nbsp; Multi-client data consistency 
	has never been an area where AFS has excelled, but our long-term experience 
	has shown this is not a necessary function of OpenAFS.&nbsp; Especially for 
	larger files, a more intelligent scheme would be to implement a relaxed consistency 
	model where synchronization could occur between the client and server at any 
	arbitrary time.&nbsp; In the case of HPC or video applications, an adaptive 
	write-behind mechanism would be the most desirable option.&nbsp; We recognize 
	that not all users desire the same semantics, so after a write-behind mechanism 
	has been implemented the next step will be to develop the ability to select 
	the data consistency model on a per-volume basis.<br>
	<br>
	In addition, write-on-close semantics are incompatible with file locks.&nbsp; 
	There is a strong desire in the AFS community to support mandatory file locking, 
	byte range locks, and optimistic locking algorithms.&nbsp; This desire is primarily 
	the result of wishing AFS to be a first class file system on the Microsoft Windows 
	and MacOS X operating systems which require those semantics.&nbsp; When locks 
	are obtained and released, the buffers affected by the locks must be flushed 
	to the file server prior to the completion of the lock release.<br>
	<br>
	The Windows OpenAFS client does not implement write-on-close semantics because 
	of the heavy use of byte range file locks.</p>
	<p style="text-indent: -18pt; margin-left: 72pt;">
	<span style="font-family: Courier New;">o<span style="font-family: &quot;Times New Roman&quot;; font-style: normal; font-variant: normal; font-weight: normal; font-size: 7pt; line-height: normal; font-size-adjust: none; font-stretch: normal;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
	</span></span>If the client cache fills up during writes, the UNIX OpenAFS client 
	blocks the process performing writes until the cache is completely flushed to 
	the low-water mark.&nbsp; A better strategy would be to dispatch the truncation 
	daemon in advance of the cache reaching a high-water mark threshold, using one 
	of the many I/O prediction algorithms available in the common literature.</p>
	<p style="text-indent: -18pt; margin-left: 72pt;">
	<span style="font-family: Courier New;">o<span style="font-family: &quot;Times New Roman&quot;; font-style: normal; font-variant: normal; font-weight: normal; font-size: 7pt; line-height: normal; font-size-adjust: none; font-stretch: normal;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
	</span></span>Read performance of both OpenAFS client implementations are hampered 
	by an inability to read more than a single chunk per RPC.&nbsp; When flushing 
	dirty buffers to the file server, the client is able to send multiple contiguous 
	chunks at a time thereby reducing the number of RPCs.&nbsp; On operating systems 
	that provide hints as to the usage patterns of the file, performance can be 
	improved by optimistically reading chunks whose need is anticipated.</p>
	<p style="text-indent: -18pt; margin-left: 72pt;">
	<span style="font-family: Courier New;">o<span style="font-family: &quot;Times New Roman&quot;; font-style: normal; font-variant: normal; font-weight: normal; font-size: 7pt; line-height: normal; font-size-adjust: none; font-stretch: normal;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
	</span></span>
	<span style="font-style: normal; font-variant: normal; font-weight: normal;">
	T</span>he restriction that no more than four RPCs may be outstanding on any 
	Rx connection is another observed bottleneck.&nbsp; This is particularly troublesome 
	when AFS is being used primarily to serve a single service such as a web server 
	as only one Rx connection is created for any collection of client, server, and 
	authentication ID.</p>
	<p>Given the limitations imposed by network processing overhead, available network 
	APIs, and system bus limitations we do not expect to achieve line rate performance 
	at network speeds past 1 Gigabit.&nbsp; We believe that performance faster than 
	1 Gigabit is achievable, but we do not know what the practical limits are beyond 
	that.&nbsp; One of our out year tasks is to investigate work being done at CERN 
	in using “off-line storage”, where an OpenAFS client bypasses the OpenAFS fileserver 
	completely and retrieves files via direct communication with disk hardware.<br>
	<br>Estimated: 20 to 25 months. <br>Implementation promised by Your File System, 2011</p>
	</li>
	<li>
	<p wrap><strong>Asynchronous RX RPCs:</strong><br>
	All Rx calls in the existing implementation are synchronous.&nbsp; The 
	currently executing thread must wait for completion.&nbsp;&nbsp; The maximum 
	number of simultaneous requests that can be processed is limited by the 
	number of threads that can be allocated to the process.&nbsp; By adding an 
	asynchronous Rx call mode, the file server can be redesigned to process 
	requests without blocking threads for callback breaks, whoareyou? probes, 
	and getcps calls.  This will permit a significant reduction in client requests
	<em>waiting for threads</em>.<br>
	<br>
	Estimated: 6 weeks for Asynchronous Rx and 3 weeks for file server modifications. 
	<br>Implementation promised by Your File System, 2011</p>
	</li>
	<li><strong>Implement Dynamic Retransmission Timeout (RTO) Algorithms for 
	RX/UDP:</strong><br>
	RX/UDP uses a fixed retransmission timeout value.  This results either in too many 
	retransmissions on 
	high latency connections or excessively long delays on high speed networks.&nbsp; 
	See RFC 2988 (TCP RTO) and RFC 2960 Section 6.3 (SCTP RTO) for suggested 
	algorithms.<br><br>Implementation promised by Your File System, 2010</li>
	<li></li>
	<li><strong>Increase RX/UDP Maximum Number of Calls:</strong><br>
	RX/UDP has a maximum of four simultaneous calls per connection.&nbsp; The 
	initial number of calls for each connection cannot be increased due to the 
	lack of an appropriate negotiation mechanism at the RX layer.&nbsp; However, 
	an application layer negotiation could be used to permit the application to 
	hint to the rx connection how many calls should be permitted.<br>
	<br>Implementation promised by Your File System, 2010<br></li>
	<li><strong>rxgk:<br>rxgk</strong> is a security class based upon the Generic Security Service Application 
	Programming Interface (GSS-API) that attempts to address a much broader range 
	of security weaknesses in AFS; not simply the use of single DES encryption ciphers.&nbsp; 
	These include issues such as:<ul>
		<li>users can impersonate the server to the cache manager since the user 
		knows the key obtained from the Kerberos service ticket</li>
		<li>neither the AFS clients nor the servers contribute any random data to 
		the construction of the key </li>
		<li>the desire to support individual keys per service per host instead of 
		a single key for all services on all hosts within a given cell</li>
		<li>the desire to provide data confidentiality and integrity protection 
		on anonymous connections as well as authenticated ones</li>
		<li>the desire to provide for algorithm agility</li>
		<li>the desire to allow the server to require the use of crypto by the clients</li>
		<li>the desire to map multiple authentication names to a single AFS ID within 
		the protection database</li>
	</ul>
	<p><strong>rxgk</strong> is designed but has not yet been fully implemented.&nbsp; Love 
	Hörnquist Åstrand, Magnus Ahltorp, Jeffrey Hutzelman, Derrick Brashear and Jeffrey 
	Altman met at KTH the week of 22 Jan 2007 to begin implementation of <strong>rxgk</strong> 
	and modify as many of the AFS services as possible.&nbsp; Love presented
	<a href="http://workshop.openafs.org/afsbpw07/talks/lha.pdf">a status report</a> 
	at the 2007 AFS &amp; Kerberos Best Practice Workshop and did more work with Derrick 
	the following week.</p>
	Implementation promised by Your File System, 2010<br>Target: 2.0<br>
	</li>
	<li><strong>rxk5:</strong><br><strong>rxk5</strong> is a Kerberos v5 based replacement for the existing security class,
	<strong>rxkad</strong>, developed by Marcus Watts (UMich) and Matt Benjamin (LinuxBox).&nbsp; 
	This is desirable because <strong>rxkad</strong> only works with DES,<br>which is an increasingly insecure choice. The goal here is to provide the highest 
	quality cryptography<br>possible using code and standards that exist today.<br>A key sub-goal of this project is to keep the code footprint as small<br>as possible. Less code should produce better code reliability,<br>and should also facilitate running this code inside a kernel<br>environment, such as the AFS cache manager.<p>
	<strong>rxk5</strong> has several minor 
	limitations:</p>
	<ul>
		<li>no support for Kerberos v5 tickets containing authorization data (aka 
		Microsoft PAC data)</li>
		<li>there is no enc-type negotiation separate from the Kerberos ticket session 
		key enc-type</li>
		<li>the client chooses the checksum type and the server has no influence</li>
		<li>it doesn&#39;t use the Kerberos v5 pseudo-random function due to limited 
		availability</li>
		<li>it requires functionality not currently exported from MIT Kerberos as 
		of release 1.6.1</li>
		<li>it cannot be implemented on platforms such as Solaris which do not ship 
		a public Kerberos API</li>
		<li>implementation on MacOS requires access to functions which are not currently 
		exported</li>
	</ul>
	<p><strong>rxk5</strong> is significantly stronger than <strong>rxkad</strong> when it comes to 
	security and requires relatively minor changes to the OpenAFS architecture.</p>
	Estimate: 1-2 months for protocol review and revision
	<br>Target: 1.8</li>
</ul>
<h2><a name="misc"></a>6. Miscellaneous</h2>
<ul>
	<li><strong>HostAFSD and Peer-to-Peer AFS:<br>
	</strong>One of the things that the Gatekeepers are frequently asked by AFS newbies 
	is &quot;why can&#39;t I share my local files by AFS?&quot;&nbsp; Many users have experienced 
	either CIFS or NFSv3 file sharing and wish to be able to do the same using AFS.&nbsp; 
	The existing AFS file server back ends store files within physical files known 
	as volumes which can be migrated, cloned, replicated to any AFS file server 
	within the same cell regardless of the operating system, partition file system, 
	or hardware.&nbsp;
	<p>For users that are willing to give up the location independence of the data, 
	there isn&#39;t much preventing the construction of a file server back end that 
	reads and writes from the native file system provided that native file system 
	has some way of notifying AFS when a file changed.&nbsp; Change notification 
	is required for the file server to be able to callback the clients and report 
	the invalidation of their data.</p>
	<p>Another question that needs to be addressed is how to provide for authenticated 
	access and access control lists.&nbsp; Finally, location discovery is a challenge 
	that might be addressed with Apple&#39;s Bonjour and/or dyndns; this work can be 
	extended to provide similar ability to discover a local cell for any client.<br>
	Estimate: 2 months</p>
	</li>
	<li><strong>Backup solutions:</strong><br>
	Backup systems in large organizations are often quite institution dependent.&nbsp; 
	When backing up AFS there are two different views.&nbsp; There is how the directory 
	tree is viewed by the end user and the AFS volume view.&nbsp; It is the mapping 
	of these two views that frequently results in differing requirements on the 
	backup systems that an organization wants to deploy.<p>Most off the shelf backup 
	systems only see file systems from the viewpoint of the user.&nbsp; Whereas 
	backing up AFS so that a given volume can be restored as needed in a location 
	independent manner is much more similar to backing up a distributed database.&nbsp; 
	Backing up the files that the database writes does not allow for the necessary 
	granularity of restores that are required.&nbsp; In addition, backing up the 
	database files while they are in use results in data inconsistencies.</p>
	<p>Teradactyl is one of the few remaining commercial offerings that have integrated 
	support for AFS. VERITAS Net Backup and Tivoli Storage Manager have both dropped 
	integrated AFS support.&nbsp; Teradactyl have been a sponsor of the AFS &amp; Kerberos 
	Best Practice Workshops for the last couple of years.<br>
	<a href="http://www.teradactyl.com/Documents/OpenAFSbasics.html">http://www.teradactyl.com/Documents/OpenAFSbasics.html</a></p>
	<p>There have also been various efforts to contribute AFS support to Amanda,
	<a href="http://www.amanda.org/">http://www.amanda.org/</a>, and there have 
	been efforts to provide an AFS wrapper to Legato Networker.</p>
	</li>
<!--	<li><strong>Instrumentation:</strong><br>
	Estimated: 2 months<br>&nbsp;</li>
-->	
	<li><strong>Object Storage:</strong><br>
	The RxOSD-extension to AFS allows to store metadata and actual filedata 
	separately. The filedata may also reside on more than one server or on 
	HSM-systems. This extension is somewhat based on the SCSI-T10-standard. A 
	file stored on OSDs is generally called an &quot;object&quot;. The AFS-fileserver is 
	extended inasmuch to serve the metadata of an object. A new server instance 
	the OSD-server is serving the actual file-data. A new server osddb server 
	finally keeps track of all registered osd-server. The osddb server is 
	somewhat comparable to the vldb-server which gives the location of a volume.&nbsp;
	<a href="http://pfanne.rzg.mpg.de/trac/openAFS-OSD/wiki/Specs">
	http://pfanne.rzg.mpg.de/trac/openAFS-OSD/wiki/Specs</a><br><br>Status: 
	Integration is in progress.<br>Target: 1.8<br>Follow up work promised by 
	Your File System, 2011.<br>&nbsp;</li>
	<li><strong>Automatic Load Balancing:</strong><br>
	CMU has a load balancing tool that could be polished and supported.<br>&nbsp;
	</li>
	<li><strong>Official Support for Cell Clones:</strong><br>
	Morgan Stanley has often
	<a href="http://www.acm.uiuc.edu/conference/2007/speakers#RonIsaacson">
	described</a> their Volume Management Service which manages clones of 
	read-only cells across multiple data centers.&nbsp; There are many things 
	that could be done to make such deployments easier to manage including 
	support for automatic failover between read only cells which are known to be 
	clones.</li>
</ul>
<h2><a name="documentation"></a>7. Documentation</h2>
<ul>
	<li>Installation and Maintenance</li>
	<li>Reference Manuals</li>
	<li>Developer Guides</li>
	<li>End User Guides</li>
</ul>
<h1 align="center"><a name="release_schedule"></a>Implementation and Schedule</h1>
<p align="left">The implementation schedule for these projects is entirely 
dependent upon resource availability.&nbsp; Please send inquiries, comments, and offers of support to
<a href="mailto:openafs-gatekeepers@openafs.org?subject=OpenAFS Roadmap Feedback">
openafs-gatekeepers@openafs.org</a>. Where external contributors have promised contributions, they are included, as are timelines when those are provided.&nbsp;&nbsp; The following release schedule is subject to change.</p>
<h2 align="left">1.4.12</h2>
<p>The next release in the stable series for UNIX is expected before the end of 
2009.&nbsp; This release will correct implementation defects and support for 
newer kernel revisions on supported operating systems.</p>
<h2>1.5.xx</h2>
<p>The 1.5 release series is used for production releases for the Microsoft 
Windows platform and provides a test release series for UNIX platforms.&nbsp; 
1.5.xx releases are issued approximately on a monthly basis.</p>
<h2>1.6</h2>
<p>The 1.6 series will replace the 1.4 series as the current stable series for 
UNIX and the 1.5 series as the current stable series for Microsoft Windows.&nbsp; 
The 1.6 series will include significant improvements to source code quality and 
one major feature change: the demand attach file server.&nbsp; Pre-release 
testing for 1.6 is expected to begin in April 2010.</p>
<h2>1.7</h2>
<p>The 1.7 series will replace the 1.5 series as the experimental release 
series.&nbsp; 1.7 releases will begin shortly after the 1.6 series enters 
pre-release testing.&nbsp; Major new features will be integrated into 1.7 
releases in preparation for the 1.8 stable release.&nbsp; April 2010.</p>
<h2>1.8</h2>
<p>The 1.8 series will replace the 1.6 series as the current stable series for 
UNIX and Microsoft Windows.&nbsp; The 1.8 series will include the rxk5 security 
class, object storage, the native AFS redirector client for Microsoft Windows, 
RxUDP performance improvements, PTS authentication name extensions, and extended 
callbacks.&nbsp; Pre-release testing for 1.8 is expected to begin in October 
2010.</p>
<h2>1.9</h2>
<p>The 1.9 series will replace the 1.7 series as the experimental release 
series.&nbsp; 1.9 releases will begin shortly after the 1.8 series enters 
pre-release testing.&nbsp; Major new features will be integrated into 1.9 
releases in preparation for the 2.0 stable release.&nbsp; October 2010.</p>
<h2>2.0</h2>
<p>The 2.0 series will replace the 1.8 series as the current stable series for 
UNIX and Microsoft Windows.&nbsp; The 1.8 series will include the rxgk security 
class including Kerberos v5, X.509 and SCRAM authentication, protection of 
anonymous connections, protection of the server to client callback connection, 
and server coordinated byte range locking, .&nbsp; Pre-release testing for 2.0 
is expected to begin in April 2011.</p>
&nbsp;<div id="footer"> 
	<hr>
	<address>
		&lt;webmaster@openafs.org&gt;</address>
	<address>
		Last modified: 2009/10/02 09:42:14 EST</address>
</div>
<script type="text/javascript">
var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
</script>
<script type="text/javascript">
var pageTracker = _gat._getTracker("UA-5995928-1");
pageTracker._trackPageview();
</script>
</body>

</html>
